{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "As a data engineer, We need to decide what would be the best approach for your company in terms of selecting tools, cost /budget, scalability of data, end user, data model, volumetric and so on\n",
    "\n",
    "First I have chosen Udacity provided dataset, added to that picked some inter-related datasets such as Airline details, Visa Details, Cost of living\n",
    "\n",
    "So now am very clear with my data and end users, Definitely this might be used by US Immigration and inter related teams.\n",
    "\n",
    "Coming to possible approaches, \n",
    "\n",
    "###### Option 1. Python, S3 for Storage, Spark for Data Processing (Eventually for Big Data), Airflow for scheduling pipelines\n",
    "\t\t  Data Model    : Star Schema\n",
    "\t\t  Cost Involve  : Yes\n",
    "\t\t  Maintenance   : Easy\n",
    "\t\t  Scalability   : Yes\n",
    "\t\t  Long Run      : Yes\n",
    "\t\t  Over all Process: Stage the data into redshift tables as same as source then after processing, can load into Redshift tables, schedule it using airflow\n",
    "\t\t  \n",
    "\n",
    "###### Option 2. Python, on-premise drives, Spark / Pandas, Juypter notebook \n",
    "\t\t  Data Model    : Star Schema\n",
    "\t\t  Cost Involve  : No\n",
    "\t\t  Maintenance   : Difficult\n",
    "\t\t  Scalability   : No\n",
    "\t\t  Long Run      : Yes\n",
    "\t\t  Over all Process: Process the data(Filtering, Cleansing, Transformation) using Spark then after processing, can save the data as Parquet files for Data Analysis\n",
    "\t\t  \t\t  \n",
    "I would like / suggest to go for option 1 since it's very reliable approach in terms of scalability, maintenance and long run\n",
    "but dropping off since Apache airflow environment does not have access to US Immigration data(6GB) and Project Workspace does not have Apache airflow.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "In this project, we have collected data from different sources such as Immigration, Temperature, Demographics, Airline and created the data model based on data assessment then performed transformation, filtering, cleansing and finally saved as Parquets file.\n",
    "\n",
    "It would help data analytics team to get answer following questions\n",
    "\n",
    "###### Tourism Dept Questions:\n",
    "1. What is the average tourist count on each month?\n",
    "2. Which is the most popular cities in US among tourister?\n",
    "3. What is the age range of tourister?\n",
    "\n",
    "###### Immigration Dept Questions:\n",
    "1. What are the top airports used by immigrants while arrival and departure?\n",
    "2. What are the preferred cities of immigrants by considering rent index?\n",
    "3. What are the preferred cities of immigrants by considering average temperature?\n",
    "\n",
    "###### Airline Dept Questions:\n",
    "1. What are the top airlines used by Immigrants?\n",
    "2. What are the top visa categories of immigrants?\n",
    "3. What is the age range of travellers by visa type?\n",
    "\n",
    "###### Demographic Dept Questions:\n",
    "1. How much percentage of immigrants adding by race category?\n",
    "2. What is the percentage of gender equality?\n",
    "3. What is the percenatge of Foreign born?  \n",
    "\n",
    "###### Metrological Dept Questions:\n",
    "1. What would be the temperture of top 5 US cities used by Immigrants in 2030? - Forecasting\n",
    "2. What is the temperature difference of top 5 US cities preferred by business Immigrants and their orgin?\n",
    "3. What are the least 5 US cities of Immigrants and its temperature?\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "| Dataset                    | Format   | Description                                                                                                                                          | Source Link                                                                        |   |\n",
    "|----------------------------|----------|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|---|\n",
    "| I94 Immigration Data       | sas7bdat | This data comes from the US National Tourism and Trade Office                                                                                        | https://travel.trade.gov/research/reports/i94/historical/2016.html                 |   |\n",
    "| World Temperature Data     | CSV      | Global Land and Ocean-and-Land Temperatures                                                                                                          | https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data |   |\n",
    "| U.S. City Demographic Data | CSV      | This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. | https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/     |   |\n",
    "| Airport Code Table         | CSV      | This is a simple table of airport codes and corresponding cities                                                                                     | https://datahub.io/core/airport-codes#data                                         |   |\n",
    "| Visa Code                  | CSV      | Describe the classes of admission                                                                                                                    | https://www.dhs.gov/immigration-statistics/nonimmigrant/NonimmigrantCOA            |   |\n",
    "| Airline Code               | CSV      | Describe Major Airline , Country, Carrier code,                                                                                                      | https://www.kwe.co.jp/en/useful-contents/code1                                     |   |\n",
    "| Cost of Living             | CSV      | Details of US States Cost of Living categorized by Rent, Groceries, Restaurants, Local Purchasing Power                                              | https://www.numbeo.com/cost-of-living/country_result.jsp?country=United+States     |   |                                    |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary packages, some with alias name for easy access\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,substring_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Entry to spark application, connect the cluster with an application\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of Immigration Data\n",
    "# Convert whole SAS Data into Parquet file as it would be easy for processing in spark\n",
    "# Preprocessing of whole 2016 data (80Million records) will take 10 - 15 mins of time\n",
    "filepath='../../data/'\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.sas7bdat'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "\n",
    "#Comment out all_files list to process whole 2016 data           \n",
    "all_files = ['/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', '/data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat']\n",
    "\n",
    "for i in all_files:\n",
    "    df_spark =spark.read.format('com.github.saurfang.sas.spark').load(i)\n",
    "    df_spark = df_spark.union(df_spark)\n",
    "\n",
    "df_spark.write.mode(\"overwrite\").parquet(\"stage\")\n",
    "print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print Schema of the immigration dataset\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "1. **Immigration Dataset**\n",
    "    * Convert the datatype to integer for cicid,i94yr,i94mon,i94cit,i94res,BIRYEAR\n",
    "    * Transform i94mode as Air, Sea, Land and Not Reported according to the values else categories as 'NA'\n",
    "    * Get the city and resident name from country table\n",
    "    * Convert the SAS date to Readable data format for Arrival and Departure date\n",
    "    * Transform i94visa as Business, Pleasure and Student according to the values otherwise catgeories as 'NA'\n",
    "    * Convert the string type to date for DTADDTO\n",
    "    * Remove duplicates\n",
    "    * Rename all the columns with proper meaningful name and avoid space or special characters\n",
    "\n",
    "\n",
    "2. **Demographics Dataset**\n",
    "    * Transpose the race row values into columns, the New columns are Whitecount, Asiancount, HispanicOrLatinocount, BlackOrAfricanAmerican, AmericanIndianandAlaskaNative\n",
    "    * Convert the float dataype to integer for Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size\n",
    "    * Remove duplicates\n",
    "    * Rename all the columns with proper meaningful name and avoid space or special characters\n",
    "\n",
    "\n",
    "3. **Temperature Dataset**\n",
    "    * Remove records having NULL temperature and NULL Dates\n",
    "    * Convert datatype timestamp to date for dt\n",
    "    * Remove duplicates\n",
    "   \n",
    "\n",
    "4. **Airport Dataset**\n",
    "    * Remove records where state='closed'\n",
    "    * Filter only US airports\n",
    "    * Remove duplicates\n",
    "    * Derive State code from iso_region\n",
    "    \n",
    "\n",
    "5. **Cost of living Dataset**\n",
    "    * Split the city and city code comma separated values into two\n",
    "    * Remove duplicates \n",
    "\n",
    "6. **Countries**\n",
    "    * Country dataset is derived from I94_SAS_Labels_Descriptions.SAS\n",
    "    * Columns are segregated with delimters =\n",
    "    * Flag column identify valid and invalid records\n",
    "\n",
    "7. **Port**\n",
    "    * Port dataset is derived from I94_SAS_Labels_Descriptions.SAS\n",
    "    * Columns are segregated with delimters ,\n",
    "    * Flag column identify valid and invalid records\n",
    "    \n",
    "    \n",
    "8. **Airline**\n",
    "    * No correction in Airline dataset\n",
    "\n",
    "\n",
    "9. **Visa**\n",
    "    * No correction in Visa dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Immigration Dataset Count 7467572\n"
     ]
    }
   ],
   "source": [
    "#Reload the immigrants dataset from stage folder\n",
    "filepath='stage/*.parquet'\n",
    "df_immigrants = spark.read.parquet(filepath)\n",
    "\n",
    "#Eliminate unwanted columns\n",
    "df_immigrants = df_immigrants.drop('count','DTADFILE','VISAPOST','OCCUP','ENTDEPA','ENTDEPD','ENTDEPU','MATFLAG','insnum','admnum')\n",
    "print(\"Total Immigration Dataset Count\", df_immigrants.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "When designing the data model, we should study the dataset first,  In our case the dataset are Immigration, temperature, demographics, Airport and supporting data's and all it has state,country as common.\n",
    "\n",
    "As i said earlier, my end user would be Immigration team and they want to analyse immigration key metrics and related data. so i have chosen star model, Immigration dataset act as fact table and rest act as dimension tables\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "![Schema](ConceptualDataModel.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CountryCode: integer (nullable = true)\n",
      " |-- CountryName: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Countries\n",
    "df_countries = spark.read.csv('Countries.csv',header=True,sep=',',inferSchema='true')\n",
    "df_countries.createOrReplaceTempView(\"countries\")\n",
    "df_countries.dropDuplicates()\n",
    "df_countries.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PortEntry: string (nullable = true)\n",
      " |-- PortName: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Port\n",
    "df_port = spark.read.csv('ports.csv',header=True,sep=',',inferSchema='true')\n",
    "df_port.createOrReplaceTempView(\"port\")\n",
    "df_port.dropDuplicates()\n",
    "df_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VisaCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visa\n",
    "df_visa = spark.read.csv('Visa Type.csv',header=True,sep=',',inferSchema='true')\n",
    "df_visa.createOrReplaceTempView(\"visa\")\n",
    "df_visa.dropDuplicates()\n",
    "df_visa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AirlineCode: integer (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CarrierCode2: string (nullable = true)\n",
      " |-- CarrierCode3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Airline\n",
    "df_airline = spark.read.csv('Airline.csv',header=True,sep=',',inferSchema='true')\n",
    "df_airline.createOrReplaceTempView(\"Airline\")\n",
    "df_airline.dropDuplicates()\n",
    "df_airline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Immigration\n",
    "df_immigrants.createOrReplaceTempView(\"immigrants\")\n",
    "immigrants = spark.sql(\"\"\"\n",
    "                            SELECT distinct \n",
    "                            int(i.cicid) as ID,\n",
    "                            int(i94yr) as ReportingYear,\n",
    "                            int(i94mon) as ReportingMonth,\n",
    "                            int(i.i94cit) as CityCode,\n",
    "                            int(i.i94res) as ResidentCode,\n",
    "                            cit.CountryName as City,\n",
    "                            res.CountryName as Resident,\n",
    "                            i.i94port as PortEntry,\n",
    "                            case when i.i94mode=1 then 'Air'\n",
    "                                 when i.i94mode=2 then 'Sea'\n",
    "                                 when i.i94mode=3 then 'Land'\n",
    "                                 when i.i94mode=9 then 'Not Reported'\n",
    "                                 Else 'NA'\n",
    "                            End as ModeofTravel,\n",
    "                            date_add('1960-01-01',arrdate) as ArrivalDate,\n",
    "                            date_add('1960-01-01',depdate) as DepartureDate,\n",
    "                            i94addr as Statecode,\n",
    "                            i94bir as Age,\n",
    "                            case when i.i94visa=1 then 'Business'\n",
    "                                 when i.i94visa=2 then 'Pleasure'\n",
    "                                 when i.i94visa=3 then 'Student'\n",
    "                                 Else 'NA'\n",
    "                            End as VisaType,\n",
    "                            visatype as VisaCode,\n",
    "                            int(BIRYEAR) as BirthYear,\n",
    "                            to_date(DTADDTO, 'MMddyyyy') as ValidTill,\n",
    "                            Gender,\n",
    "                            i.airline as CarrierCode2,\n",
    "                            fltno as FlightNumber\n",
    "                            FROM immigrants i \n",
    "                            join countries cit on int(i.i94cit)=cit.CountryCode and cit.flag='Valid'\n",
    "                            join countries res on int(i.i94res)=res.CountryCode and cit.flag='Valid'\n",
    "                        \"\"\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3254348"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the new modifed immigration schema\n",
    "print(\"Processing...\")\n",
    "print(\"Total Valid immigrants\",immigrants.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------+--------+------------+-------+--------+---------+------------+-----------+-------------+---------+----+--------+--------+---------+----------+------+------------+------------+\n",
      "|    ID|ReportingYear|ReportingMonth|CityCode|ResidentCode|   City|Resident|PortEntry|ModeofTravel|ArrivalDate|DepartureDate|Statecode| Age|VisaType|VisaCode|BirthYear| ValidTill|Gender|CarrierCode2|FlightNumber|\n",
      "+------+-------------+--------------+--------+------------+-------+--------+---------+------------+-----------+-------------+---------+----+--------+--------+---------+----------+------+------------+------------+\n",
      "|864993|         2016|             9|     108|         108|DENMARK| DENMARK|      FTL|         Air| 2016-09-04|   2016-09-05|       FL|53.0|Pleasure|      WT|     1963|2016-12-02|     F|          DY|        7041|\n",
      "+------+-------------+--------------+--------+------------+-------+--------+---------+------------+-----------+-------------+---------+----+--------+--------+---------+----------+------+------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigrants.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original File Count 2891\n"
     ]
    }
   ],
   "source": [
    "#Demographics\n",
    "df_demographics = spark.read.csv('us-cities-demographics.csv',header=True,sep=';',inferSchema='true')\n",
    "print(\"Original File Count\",df_demographics.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record count after transpose race into columns 596\n"
     ]
    }
   ],
   "source": [
    "df_demographics.createOrReplaceTempView(\"demographics\")\n",
    "demographics = spark.sql(\"\"\"Select temp.State, \n",
    "                                State_Code as StateCode,\n",
    "                                temp.City,\n",
    "                                Avg(Median_Age) as MedianAge,\n",
    "                                int(Avg(Male_Population)) as MalePopulation,\n",
    "                                int(Avg(Female_Population)) as FemalePopulation,\n",
    "                                int(Avg(Total_Population)) as TotalPopulation,\n",
    "                                int(Avg(NumberofVeterans)) as NumberofVeterans, \n",
    "                                int(Avg(Foreign_born)) as ForeignBorn, \n",
    "                                int(Avg(Average_Household_Size)) as AverageHouseholdSize, \n",
    "                                Sum(Whitecount) as White,\n",
    "                                Sum(Asiancount) as Asian, \n",
    "                                Sum(HispanicOrLatinocount) as HispanicOrLatino ,\n",
    "                                Sum(AmericanIndianandAlaskaNative) as AmericanIndianandAlaskaNative,\n",
    "                                Sum(BlackOrAfricanAmerican) as BlackOrAfricanAmerican\n",
    "                            from\n",
    "                            (SELECT state,city,`Median Age` as Median_Age\n",
    "                            ,`Male Population` as Male_Population,`Female Population` as Female_Population\n",
    "                            ,`Total Population` as Total_Population\n",
    "                            ,`Number of Veterans` as NumberofVeterans\n",
    "                            ,`Foreign-born` as Foreign_born\n",
    "                            ,`Average Household Size` as Average_Household_Size\n",
    "                            ,`State Code` as State_Code\n",
    "                            ,case when race='White' then count else 0\n",
    "                             end as Whitecount\n",
    "                             ,case when race='Asian' then count else 0\n",
    "                             end as Asiancount\n",
    "                             ,case when race='Hispanic or Latino' then count else 0\n",
    "                             end as HispanicOrLatinocount\n",
    "                             ,case when race='Black or African-American' then count else 0\n",
    "                             end as BlackOrAfricanAmerican\n",
    "                             ,case when race='American Indian and Alaska Native' then count else 0\n",
    "                             end as AmericanIndianandAlaskaNative\n",
    "                            FROM demographics ) as temp\n",
    "                            group by temp.state,temp.city,temp.Median_Age,temp.Male_Population,temp.Female_Population\n",
    "                            ,temp.Total_Population,temp.NumberofVeterans ,temp.Foreign_born,temp.Average_Household_Size\n",
    "                            ,temp.State_Code\n",
    "                            \"\"\")\n",
    "print(\"Total Record count after transpose race into columns\", demographics.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- StateCode: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- MedianAge: double (nullable = true)\n",
      " |-- MalePopulation: integer (nullable = true)\n",
      " |-- FemalePopulation: integer (nullable = true)\n",
      " |-- TotalPopulation: integer (nullable = true)\n",
      " |-- NumberofVeterans: integer (nullable = true)\n",
      " |-- ForeignBorn: integer (nullable = true)\n",
      " |-- AverageHouseholdSize: integer (nullable = true)\n",
      " |-- White: long (nullable = true)\n",
      " |-- Asian: long (nullable = true)\n",
      " |-- HispanicOrLatino: long (nullable = true)\n",
      " |-- AmericanIndianandAlaskaNative: long (nullable = true)\n",
      " |-- BlackOrAfricanAmerican: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------+---------+--------------+----------------+---------------+----------------+-----------+--------------------+-----+-----+----------------+-----------------------------+----------------------+\n",
      "|  State|StateCode|         City|MedianAge|MalePopulation|FemalePopulation|TotalPopulation|NumberofVeterans|ForeignBorn|AverageHouseholdSize|White|Asian|HispanicOrLatino|AmericanIndianandAlaskaNative|BlackOrAfricanAmerican|\n",
      "+-------+---------+-------------+---------+--------------+----------------+---------------+----------------+-----------+--------------------+-----+-----+----------------+-----------------------------+----------------------+\n",
      "|Florida|       FL|Miami Gardens|     34.9|         50719|           62480|         113199|            2327|      33394|                   3|27273|    0|           23287|                            0|                 85300|\n",
      "+-------+---------+-------------+---------+--------------+----------------+---------------+----------------+-----------+--------------------+-----+-----+----------------+-----------------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographics.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Temparature \n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature =spark.read.csv(fname,header=True,sep=',',inferSchema='true')\n",
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Temperature Records 8599212\n",
      "After eliminating invalid date and temperature 8235082\n"
     ]
    }
   ],
   "source": [
    "#Only filtering valid date and valid temperature\n",
    "print(\"Total Temperature Records\",df_temperature.count())\n",
    "df_temperature.createOrReplaceTempView(\"temperature\")\n",
    "temperature = spark.sql(\"\"\" SELECT distinct *\n",
    "                            FROM temperature \n",
    "                            where dt is not null \n",
    "                            and AverageTemperature is not null\n",
    "                        \"\"\")\n",
    "print(\"After eliminating invalid date and temperature\",temperature.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- CostofLivingIndex: double (nullable = true)\n",
      " |-- RentIndex: double (nullable = true)\n",
      " |-- CostofLivingPlusRentIndex: double (nullable = true)\n",
      " |-- GroceriesIndex: double (nullable = true)\n",
      " |-- RestaurantPriceIndex: double (nullable = true)\n",
      " |-- LocalPurchasingPowerIndex: double (nullable = true)\n",
      " |-- CityCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cost of living \n",
    "df_col =spark.read.csv('Cost of Living.csv',header=True,sep=',',inferSchema='true')\n",
    "\n",
    "#Derived city code from city\n",
    "df_col = df_col.withColumn(\"CityCode\", substring_index(col(\"City\"), \",\", -1))\n",
    "df_col = df_col.withColumn(\"City\", substring_index(col(\"City\"), \",\", 1))\n",
    "df_col.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Airport \n",
    "df_airport =spark.read.csv('airport-codes_csv.csv',header=True,sep=',',inferSchema='true')\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Record Count 55075\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Record Count\",df_airport.count())\n",
    "df_airport.dropDuplicates()\n",
    "df_airport = df_airport.withColumn(\"CityCode\", substring_index(col(\"iso_region\"), \"-\", -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+--------+\n",
      "|ident|    type|             name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|CityCode|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+--------+\n",
      "|  00A|heliport|Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|      PA|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport.createOrReplaceTempView(\"airport\")\n",
    "airport = spark.sql(\"\"\" SELECT *\n",
    "                            FROM airport \n",
    "                            where iso_country='US' and type <> 'closed'\n",
    "                    \"\"\")\n",
    "airport.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records in countries table 289\n",
      "Total Records in visa table 90\n",
      "Total Records in port table 660\n",
      "Total Records in immigrants table 3254348\n",
      "Total Records in demographics table 596\n",
      "Total Records in temperature table 8599212\n",
      "Total Records in airline table 61\n",
      "Total Records in cost of living table 63\n",
      "Total Records in airport table 21431\n"
     ]
    }
   ],
   "source": [
    "#Basic checks on record count before writing as file or loading into table\n",
    "print(\"Total Records in countries table\",df_countries.count())\n",
    "print(\"Total Records in visa table\",df_visa.count())\n",
    "print(\"Total Records in port table\",df_port.count())\n",
    "print(\"Total Records in immigrants table\",immigrants.count())\n",
    "print(\"Total Records in demographics table\",demographics.count())\n",
    "print(\"Total Records in temperature table\",df_temperature.count())\n",
    "print(\"Total Records in airline table\",df_airline.count())\n",
    "print(\"Total Records in cost of living table\",df_col.count())\n",
    "print(\"Total Records in airport table\",airport.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|i94mon|\n",
      "+------+\n",
      "|   9.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Immigration Validation\n",
    "filepath='stage/*.parquet'\n",
    "df_immigrants = spark.read.parquet(filepath)\n",
    "df_immigrants.createOrReplaceTempView(\"immigration\")#print(all_files)\n",
    "immigration_validation = spark.sql(\"\"\"\n",
    "                            SELECT  distinct i94mon \n",
    "                            FROM immigration\n",
    "                            \"\"\")\n",
    "immigration_validation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#writing output to parquet files\n",
    "output_data='out/'\n",
    "print(\"Writing back to parquet file...\")\n",
    "immigrants.write.mode(\"overwrite\").parquet(output_data+\"fact/\")\n",
    "df_countries.write.mode(\"overwrite\").parquet(output_data+\"country/\")\n",
    "df_visa.write.mode(\"overwrite\").parquet(output_data+\"visa/\")\n",
    "df_port.write.mode(\"overwrite\").parquet(output_data+\"port/\")\n",
    "demographics.write.mode(\"overwrite\").parquet(output_data+\"demographics/\")\n",
    "df_temperature.write.mode(\"overwrite\").parquet(output_data+\"temperature/\")\n",
    "df_airline.write.mode(\"overwrite\").parquet(output_data+\"airline/\")\n",
    "df_col.write.mode(\"overwrite\").parquet(output_data+\"costofliving/\")\n",
    "airport.write.mode(\"overwrite\").parquet(output_data+\"airport/\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "[Data Dictionary Link](https://r766466c839826xjupyterlnnfq3jud.udacity-student-workspaces.com/lab/tree/Data%20Dictionary.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "##### Tools and technologies\n",
    "As explained earlier, we have selected option 2 for this project as tools and technologies involved are open source and no cost strategy\n",
    "\n",
    "Option 2. Python, on-premise drives, Spark / Pandas, Juypter notebook\n",
    "\n",
    "      Data Model    : Star Schema\n",
    "      Cost Involve  : No\n",
    "      Maintenance   : Difficult\n",
    "      Scalability   : No\n",
    "      Long Run      : Yes\n",
    "      Over all Process: Process the data(Filtering, Cleansing, Transformation) using Spark then after processing, can save the data as Parquet files for Data Analysis\n",
    "\n",
    "##### Propose how often the data should be updated and why      \n",
    "* When considering the whole dataset used in this project, the data frequency should be monthly.\n",
    "\n",
    "#### Possible Scenario's:\n",
    " * ###### In case the data was increased by 100x.\n",
    "    We can go for the apporach 1, using Redshift, EMR cluster and Airflow would be able to manage scalabilty of data\n",
    " * ###### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     By chance if we get daily, then we can schedule the airflow job to run prior to 5hrs of dashboard refresh \n",
    " * ###### The database needed to be accessed by 100+ people.\n",
    "    Yes we can make it by increasing red shift cluster instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
